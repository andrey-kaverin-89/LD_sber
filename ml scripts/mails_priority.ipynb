{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:41:22.291862Z",
     "start_time": "2020-11-28T15:41:22.285505Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from pymystem3 import Mystem\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:37:24.327126Z",
     "start_time": "2020-11-28T15:37:24.321188Z"
    }
   },
   "outputs": [],
   "source": [
    "def stemming(tokens: list) -> pd.Series:\n",
    "    stem = Mystem()\n",
    "    tokens = [word_tokenize(\"\".join(stem.lemmatize(sentence))) for sentence in tokens]\n",
    "    tokens = [[word for word in sentence if len(word) > 2] for sentence in tokens]\n",
    "#     tokens = [\n",
    "#         [\n",
    "#             word\n",
    "#             for word in sentence\n",
    "#             if morph.parse(word)[0].tag.POS == \"NOUN\"\n",
    "#             or morph.parse(word)[0].tag.POS == \"ADJF\"\n",
    "#         ]\n",
    "#         for sentence in tokens\n",
    "#     ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:37:33.138899Z",
     "start_time": "2020-11-28T15:37:33.134217Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_stopwords(text: pd.Series) -> pd.Series:\n",
    "    result = [\n",
    "        [word for word in sentence if word not in ru_stopwords and word != \" \"]\n",
    "        for sentence in text\n",
    "    ]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:37:45.314622Z",
     "start_time": "2020-11-28T15:37:45.309948Z"
    }
   },
   "outputs": [],
   "source": [
    "ru_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:42:25.052595Z",
     "start_time": "2020-11-28T15:42:25.041992Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('labeled_mails.csv', index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:43:39.900155Z",
     "start_time": "2020-11-28T15:43:39.889732Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df.text.apply(lambda x: re.sub('[0-9]+', '', x))\n",
    "df['clean_text'] = df.clean_text.apply(lambda x: re.sub('[^а-яА-Я]+', ' ', x))\n",
    "df['clean_text'] = df.clean_text.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:44:23.508563Z",
     "start_time": "2020-11-28T15:44:23.499474Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[13, 'text'] = None\n",
    "df = df[~df.text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:44:24.835315Z",
     "start_time": "2020-11-28T15:44:24.233477Z"
    }
   },
   "outputs": [],
   "source": [
    "df.clean_text = stemming(df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:44:25.204260Z",
     "start_time": "2020-11-28T15:44:25.188441Z"
    }
   },
   "outputs": [],
   "source": [
    "df.clean_text = check_stopwords(df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:49:35.459891Z",
     "start_time": "2020-11-28T15:49:35.454975Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.clean_text.reset_index(drop=True)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:49:35.989323Z",
     "start_time": "2020-11-28T15:49:35.984181Z"
    }
   },
   "outputs": [],
   "source": [
    "X = X.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:49:36.533285Z",
     "start_time": "2020-11-28T15:49:36.529608Z"
    }
   },
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:49:36.874450Z",
     "start_time": "2020-11-28T15:49:36.870487Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:49:37.093042Z",
     "start_time": "2020-11-28T15:49:37.078479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 896 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(X.values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:49:37.562507Z",
     "start_time": "2020-11-28T15:49:37.550776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (29, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:50:12.377430Z",
     "start_time": "2020-11-28T15:50:12.370494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (29, 4)\n"
     ]
    }
   ],
   "source": [
    "y = pd.get_dummies(y).values\n",
    "print('Shape of label tensor:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T15:51:03.752126Z",
     "start_time": "2020-11-28T15:51:03.720153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 1.3862 - accuracy: 0.1538 - val_loss: 1.3918 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.3747 - accuracy: 0.5769 - val_loss: 1.3851 - val_accuracy: 0.6667\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.3595 - accuracy: 0.6923 - val_loss: 1.3775 - val_accuracy: 0.6667\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 1.3473 - accuracy: 0.6538 - val_loss: 1.3683 - val_accuracy: 0.6667\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3292 - accuracy: 0.6538 - val_loss: 1.3568 - val_accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X, y, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
